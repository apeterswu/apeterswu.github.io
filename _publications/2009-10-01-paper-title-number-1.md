---
title: "Sequence Generation with Mixed Representations"
collection: publications
permalink: /publication/2009-10-01-paper-title-number-1
excerpt: ''
date: 2009-10-01
author: <b>Lijun Wu</b>, Jinhua Zhu
conference: In Thirty-seventh International Conference on Machine Learning (ICML-2020)
venue: ''
paperurl: 'http://xxx.github.io/files/paper1.pdf'
citation: '<br>
@inproceedings{wu2018adversarial, <br>
  title={Adversarial neural machine translation}, <br>
  author={Wu, Lijun and Xia, Yingce and Tian, Fei and Zhao, Li and Qin, Tao and Lai, Jianhuang and Liu, Tie-Yan}, <br>
  booktitle={Asian Conference on Machine Learning}, <br>
  pages={534--549}, <br>
  year={2018} <br>
}'


---
<h2><strong>Abstract</strong></h2>
Tokenization is the first step of many natural language processing (NLP) tasks and plays an important role for neural NLP models. Tokenization methods such as byte-pair encoding and SentencePiece, which can greatly reduce the large vocabulary size and deal with out-of-vocabulary words, have shown to be effective and are widely adopted for sequence generation tasks. While various tokenization methods exist, there is no common acknowledgement which one is the best. In this work, we propose to leverage the mixed representations from different tokenizers for sequence generation tasks, which can take the advantages of each individual tokenization method.
Specifically, we introduce a new model architecture to incorporate mixed representations and a co-teaching algorithm to better utilize the diversity of different tokenization methods. Our approach achieves significant improvements on neural machine translation tasks with six language pairs, as well as an abstractive summarization task.

\[[PDF](http://xxx.github.io/files/paper1.pdf)\]  \[[CODE](http://xxx.github.io/files/paper1.pdf)\] 
